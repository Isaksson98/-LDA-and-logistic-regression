i = i + 1
}
df <- data.frame(tpr=c(tpr_tree,tpr_bayes), fpr=c(fpr_tree, fpr_bayes),
Method=c(rep(paste("Optimal Tree"), each=length(tpr_tree)),
rep(paste("NaÃ¯ve Bayes"), each=length(tpr_bayes))) )
ggplot(aes(x=fpr, y=tpr, color=Method), data=df) +
geom_line() + xlim(0,1) + ylim(0,1) +
xlab('fpr') + ylab('tpr')
fit_default = tree(formula=y~., data=train)
fit_default
fita = tree(y ~ ., data=train)
fita
predicta = predict(fita, newdata=valid, type='class')
predict_default = predict(fit_default, new_data=valid, type='class')
predicta
predict_default
predict_default=predicta
predict_default==predicta
predict_default = predict(fit_default, new_data=valid, type='class')
predicta = predict(fita, newdata=valid, type='class')
predict_default==predicta
fita==fit_default
predicta2 = predict(fit_default, newdata=valid, type='class')
predicta==predicta2
predict_default = predict(fit_default, newdata=valid, type='class')
set.seed(12345)
library(tree)
data = read.csv2('bank-full.csv', header=TRUE, stringsAsFactors = TRUE)
data$duration = NULL
head(data)
#Partion data ####
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
#Fit decision tree to data####
control1 = tree.control(nobs=dim(train)[1], minsize=7000)
control2 = tree.control(nobs=dim(train)[1], mindev=0.0005)
fit_default = tree(formula=y~., data=train)
fit_node_size = tree(formula= y~., data=train, control=control1)
fit_deviance = tree(formula= y~., data=train, control=control2)
predict_default = predict(fit_default, newdata=valid, type='class')
predict_node_size = predict(fit_node_size, newdata=valid, type='class')
predict_deviance = predict(fit_deviance, newdata=valid, type='class')
missclass1 = 1-sum(diag(table(valid$y,predict_default))/sum(table(valid$y,predict_default)))
missclass2 = mean(predict_node_size != valid$y)
missclass3 = mean(predict_deviance != valid$y)
missclass1
missclass2
missclass3
#Find optimal depth####
train_dev = rep(NA, 50)
valid_dev = rep(NA, 50)
for (i in 2:50){
pruned_tree = prune.tree(fit_deviance, best=i)
pred = predict(pruned_tree, new_data=valid, type='tree')
train_dev[i]=deviance(pruned_tree)
valid_dev[i]=deviance(pred)
}
plot(valid_dev, col='red')
points(train_dev, col='blue')
valid_dev
train_dev
for (i in 2:50){
pruned_tree = prune.tree(fit_deviance, best=i)
pred = predict(pruned_tree, newdata=valid, type='tree')
train_dev[i]=deviance(pruned_tree)
valid_dev[i]=deviance(pred)
}
plot(valid_dev, col='red')
points(train_dev, col='blue')
valid_dev
train_dev
fit = rpart(y ~ ., data=test, parms = list(loss = loss_matrix))
data = read.csv('communities.csv')
head(data)
data = read.csv('communities.csv')
scaled_data = scale(data[, data$ViolentCrimesPerPop])
head(scaled_data)
head(data)
data[, 1:100]
dim(data)
original_data = read.csv('communities.csv')
data = original_data[,1:100]
data = scale(data)
head(data)
n=dim(data)[1]
S = 1/n*t(data)*data
S = 1/n*t(data)%*%data
S
eig = eigen(S)
eig
eig_values = eig$values
eig_values
variance_percentage = eig_values/sum(eig_values)*100
variance_percentage
temp = 0
i=0
while (temp < 0.95){
temp = sum(variance_percentage[1:i])
i=i+1
temp
}
i
temp
temp = 0
i=0
while (temp < 0.95){
temp = sum(variance_percentage[1:i])/100
i=i+1
temp
}
temp
i
variance_percentage[1:2]
PCA = princomp(data)
screeplot(PCA)
U = PCA$loadings
U
U
U[,1]
U1_vec = as.vector(U1)
U = PCA$loadings
U1 = U[,1]
U1_vec = as.vector(U1)
U1_vec
plot(U1)
tail(sort(abs(U1)),5)
U1
tail(sort(abs(U1)),5)
x = data.frame(pc1=res$scores[,1], pc2=res$scores[,2], crimes=origin_data$ViolentCrimesPerPop)
x = data.frame(pc1=PCA$scores[,1], pc2=PCA$scores[,2], crimes=origin_data$ViolentCrimesPerPop)
x = data.frame(pc1=PCA$scores[,1], pc2=PCA$scores[,2], crimes=data$ViolentCrimesPerPop)
x = data.frame(pc1=PCA$scores[,1], pc2=PCA$scores[,2], crimes=original_data$ViolentCrimesPerPop)
plot(x)
original_data = read.csv('communities.csv')
data = original_data[,1:100]
data = scale(data)
n=dim(data)[1]
S = 1/n*t(data)%*%data
eig = eigen(S)
eig_values = eig$values
variance_percentage = eig_values/sum(eig_values)*100
temp = 0
i=0
while (temp < 0.95){
temp = sum(variance_percentage[1:i])/100
i=i+1
}
PCA = princomp(data)
screeplot(PCA)
U = PCA$loadings
U1 = U[,1]
U1_vec = as.vector(U1)
library(datasets)
library(MASS) #LDA
library(nnet)
library(mvtnorm)
library(caret)
data(iris)
summary(iris)
set.seed(12345)
dimen =function(x_){
print(NROW(x_))
print(NCOL(x_))
}
softmax =function(x){
num1 = exp(x)
num2 = sum(exp(x))
return (num1/num2)
}
plot(iris$Sepal.Length, iris$Sepal.Width,col = iris$Species, main="Iris Data")
#legend(5,5,unique(iris$Species),col=1:length(iris$Species),pch=1)
#Setosa will be easy to classify using LDA since they are clearly
#seperated. Versicolor and viginica have no clear border between
#them and LDA will be harder.
mean1_len = mean(iris$Sepal.Length[iris$Species=='setosa'])
mean2_len = mean(iris$Sepal.Length[iris$Species=='versicolor'])
mean3_len = mean(iris$Sepal.Length[iris$Species=='virginica'])
mean1_wid = mean(iris$Sepal.Width[iris$Species=='setosa'])
mean2_wid = mean(iris$Sepal.Width[iris$Species=='versicolor'])
mean3_wid = mean(iris$Sepal.Width[iris$Species=='virginica'])
mean1 <- c(mean1_len, mean1_wid)
mean2 <- c(mean2_len, mean2_wid)
mean3 <- c(mean3_len, mean3_wid)
N1 = length(iris$Species[iris$Species=='setosa'])
N2 = length(iris$Species[iris$Species=='versicolor'])
N3 = length(iris$Species[iris$Species=='virginica'])
pi_1 = N1 / length(iris$Species)
pi_2 = N2 / length(iris$Species)
pi_3 = N3 / length(iris$Species)
#Covariance matricies
(cov(subset(iris,subset=Species=='setosa',select=-Species)) -> S1)
(cov(subset(iris,subset=Species=='versicolor',select=-Species)) -> S2)
(cov(subset(iris,subset=Species=='virginica',select=-Species)) -> S3)
#cov(x = [x1, x2])
x = matrix(c(iris$Sepal.Length,iris$Sepal.Width),nrow=length(iris$Sepal.Length))
overall_cov = ginv((S1[1:2,1:2]*N1 + S2[1:2,1:2]*N2 + S3[1:2,1:2]*N3)/sum(N1,N2, N3))
w01 = (-1/2)*t(mean1)%*%overall_cov%*%mean1 + log(pi_1)
w1 = overall_cov  %*% mean1
disc1 = (x%*% w1) + w01[1,1]
w02 = (-1/2)*t(mean2)%*%overall_cov%*%mean2 + log(pi_2)
w2 = overall_cov  %*% mean2
disc2 = (x%*% w2) + w02[1,1]
w03 = (-1/2)*t(mean3)%*%overall_cov%*%mean3 + log(pi_3)
w3 = overall_cov  %*% mean3
disc3 = (x%*% w3) + w03[1,1]
prediction =function(disc1_, disc2_, disc3_){
#disc <- cbind(softmax(disc1_), softmax(disc2_), softmax(disc3_))
disc <- cbind(disc1_, disc2_, disc3_)
num2 = cbind(1:nrow(disc), max.col(disc, 'first'))
return (num2[,2])
}
num2 = prediction(disc1, disc2, disc3)
#, xlim=c(0,10), ylim=c(0,10)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", xlim=c(0,10), ylim=c(0,10))
table(iris[,5],num2)
#Solve: w3*x+w03 = w1*x+w01
p12 = (w02[1,1]-w01[1,1])*ginv(w1-w2)
abline(p12[2], p12[1])
p13 = (w03[1,1]-w01[1,1])*ginv(w1-w3)
abline(p13[2], p13[1])
p23 = (w02[1,1]-w03[1,1])*ginv(w3-w2)
abline(p13[2], p13[1])
#Fit the model
model = lda( iris$Species ~ iris$Sepal.Length + iris$Sepal.Width, iris)
# Make predictions
predictions = predict(model, newdata=iris[,c(1,2,3,4)])$class
# Model accuracy
table(iris[,5],predictions)
#Q4
num5 = cbind(rmvnorm(50, mean = mean1, sigma = S1[1:2,1:2]), 1)
num6 = cbind(rmvnorm(50, mean = mean2, sigma = S2[1:2,1:2]), 2)
num7 = cbind(rmvnorm(50, mean = mean3, sigma = S3[1:2,1:2]), 3)
new_data = rbind(num5, num6, num7)
plot( new_data[,1], new_data[,2],
col = new_data[,3],
xlab="Sepal Length", ylab="Sepal Width",
main="Generated data")
#Q5
model_nnet = multinom(iris$Species ~ iris$Sepal.Length + iris$Sepal.Width,
iris)
nnet_predictions <- predict(model_nnet,iris,type = "class")
table(iris$Species,nnet_predictions)
cfm1 <- confusionMatrix(iris$Species, nnet_predictions)
cfm2 <- confusionMatrix(iris$Species, predictions)
plot(iris$Sepal.Length, iris$Sepal.Width,col = nnet_predictions, main="NNET")
library(datasets)
library(MASS) #LDA
library(nnet)
library(mvtnorm)
library(caret)
data(iris)
summary(iris)
set.seed(12345)
dimen =function(x_){
print(NROW(x_))
print(NCOL(x_))
}
softmax =function(x){
num1 = exp(x)
num2 = sum(exp(x))
return (num1/num2)
}
plot(iris$Sepal.Length, iris$Sepal.Width,col = iris$Species, main="Iris Data")
#legend(5,5,unique(iris$Species),col=1:length(iris$Species),pch=1)
#Setosa will be easy to classify using LDA since they are clearly
#seperated. Versicolor and viginica have no clear border between
#them and LDA will be harder.
mean1_len = mean(iris$Sepal.Length[iris$Species=='setosa'])
mean2_len = mean(iris$Sepal.Length[iris$Species=='versicolor'])
mean3_len = mean(iris$Sepal.Length[iris$Species=='virginica'])
mean1_wid = mean(iris$Sepal.Width[iris$Species=='setosa'])
mean2_wid = mean(iris$Sepal.Width[iris$Species=='versicolor'])
mean3_wid = mean(iris$Sepal.Width[iris$Species=='virginica'])
mean1 <- c(mean1_len, mean1_wid)
mean2 <- c(mean2_len, mean2_wid)
mean3 <- c(mean3_len, mean3_wid)
N1 = length(iris$Species[iris$Species=='setosa'])
N2 = length(iris$Species[iris$Species=='versicolor'])
N3 = length(iris$Species[iris$Species=='virginica'])
pi_1 = N1 / length(iris$Species)
pi_2 = N2 / length(iris$Species)
pi_3 = N3 / length(iris$Species)
#Covariance matricies
(cov(subset(iris,subset=Species=='setosa',select=-Species)) -> S1)
(cov(subset(iris,subset=Species=='versicolor',select=-Species)) -> S2)
(cov(subset(iris,subset=Species=='virginica',select=-Species)) -> S3)
#cov(x = [x1, x2])
x = matrix(c(iris$Sepal.Length,iris$Sepal.Width),nrow=length(iris$Sepal.Length))
overall_cov = ginv((S1[1:2,1:2]*N1 + S2[1:2,1:2]*N2 + S3[1:2,1:2]*N3)/sum(N1,N2, N3))
w01 = (-1/2)*t(mean1)%*%overall_cov%*%mean1 + log(pi_1)
w1 = overall_cov  %*% mean1
disc1 = (x%*% w1) + w01[1,1]
w02 = (-1/2)*t(mean2)%*%overall_cov%*%mean2 + log(pi_2)
w2 = overall_cov  %*% mean2
disc2 = (x%*% w2) + w02[1,1]
w03 = (-1/2)*t(mean3)%*%overall_cov%*%mean3 + log(pi_3)
w3 = overall_cov  %*% mean3
disc3 = (x%*% w3) + w03[1,1]
prediction =function(disc1_, disc2_, disc3_){
#disc <- cbind(softmax(disc1_), softmax(disc2_), softmax(disc3_))
disc <- cbind(disc1_, disc2_, disc3_)
num2 = cbind(1:nrow(disc), max.col(disc, 'first'))
return (num2[,2])
}
num2 = prediction(disc1, disc2, disc3)
#, xlim=c(0,10), ylim=c(0,10)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", xlim=c(0,10), ylim=c(0,10))
table(iris[,5],num2)
#Solve: w3*x+w03 = w1*x+w01
p12 = (w02[1,1]-w01[1,1])*ginv(w1-w2)
abline(p12[2], p12[1])
p13 = (w03[1,1]-w01[1,1])*ginv(w1-w3)
abline(p13[2], p13[1])
p23 = (w02[1,1]-w03[1,1])*ginv(w3-w2)
abline(p13[2], p13[1])
#Fit the model
model = lda( iris$Species ~ iris$Sepal.Length + iris$Sepal.Width, iris)
# Make predictions
predictions = predict(model, newdata=iris[,c(1,2,3,4)])$class
# Model accuracy
table(iris[,5],predictions)
#Q4
num5 = cbind(rmvnorm(50, mean = mean1, sigma = S1[1:2,1:2]), 1)
num6 = cbind(rmvnorm(50, mean = mean2, sigma = S2[1:2,1:2]), 2)
num7 = cbind(rmvnorm(50, mean = mean3, sigma = S3[1:2,1:2]), 3)
new_data = rbind(num5, num6, num7)
plot( new_data[,1], new_data[,2],
col = new_data[,3],
xlab="Sepal Length", ylab="Sepal Width",
main="Generated data")
#Q5
model_nnet = multinom(iris$Species ~ iris$Sepal.Length + iris$Sepal.Width,
iris)
nnet_predictions <- predict(model_nnet,iris,type = "class")
table(iris$Species,nnet_predictions)
cfm1 <- confusionMatrix(iris$Species, nnet_predictions)
cfm2 <- confusionMatrix(iris$Species, predictions)
plot(iris$Sepal.Length, iris$Sepal.Width,col = nnet_predictions, main="NNET")
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", xlim=c(0,10), ylim=c(0,10))
table(iris[,5],num2)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction")
table(iris[,5],num2)
w01 = (-1/2)*t(mean1)%*%overall_cov%*%mean1 + log(pi_1)
w1 = overall_cov  %*% mean1
disc1 = (x%*% w1) + w01[1,1]
w02 = (-1/2)*t(mean2)%*%overall_cov%*%mean2 + log(pi_2)
w2 = overall_cov  %*% mean2
disc2 = (x%*% w2) + w02[1,1]
w03 = (-1/2)*t(mean3)%*%overall_cov%*%mean3 + log(pi_3)
w3 = overall_cov  %*% mean3
disc3 = (x%*% w3) + w03[1,1]
disc1
w01
w1
w02
w2
w03
w3
w03[1,1]
p12
ginv(w1-w2)
k = w02[1,1]-w01[1,1]
m = w1-w2
k
m
m = w02[1,1]-w01[1,1]
m
k = w2-w1
k
m = w02[1,1]-w01[1,1]
k = w2-w1
abline(m, k)
k=k[1]/k[2]
k
abline(m, k)
k_ = w1-w2
k=k[1]/k[2]
k
k_ = w1-w2
k=k_[1]/k_[2]
k
m = w02[1,1]-w01[1,1]
m
abline(m, -k)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", ylim=c(0,10),xlim=c(0,10))
table(iris[,5],num2)
m = w02[1,1]-w01[1,1]
k_ = w1-w2
k=k_[1]/k_[2]
abline(m, -k)
abline(4, -k)
abline(m, -k)
m = w01[1,1]-w02[1,1]
k_ = w1-w2
k=k_[1]/k_[2]
abline(m, -k)
m
w1
w2
k_
m = w01[1,1]-w03[1,1]
k_ = w1-w3
k=k_[1]/k_[2]
abline(m, -k)
k
abline(m, -k)
m = w03[1,1]-w02[1,1]
k_ = w3-w2
k=k_[1]/k_[2]
abline(m, -k)
k
m = (w01[1,1]-w03[1,1])
m
k=k_[1]/k_[2]
k
k_ = w1-w2
k=k_[1]/k_[2]
k
m = w01[1,1]-w02[1,1]
m
k
k_
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", ylim=c(0,10),xlim=c(0,10))
table(iris[,5],num2)
#Solve: w3*x+w03 = w1*x+w01
k_ = w1-w2
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
abline(m, -k)
k_ = w1-w3
k=k_[1]/k_[2]
m = (w01[1,1]-w03[1,1])/k_[2]
abline(m, -k)
k_ = w3-w2
k=k_[1]/k_[2]
m = (w03[1,1]-w02[1,1])/k_[2]
abline(m, -k)
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
k
m
k_ = w1-w2
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
abline(m, -k)
k
m
#, xlim=c(0,10), ylim=c(0,10)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction", ylim=c(0,10),xlim=c(0,10))
table(iris[,5],num2)
#Solve: w3*x+w03 = w1*x+w01
k_ = w1-w2
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
k_ = w1-w3
k=k_[1]/k_[2]
m = (w01[1,1]-w03[1,1])/k_[2]
abline(-m, -k)
k_ = w3-w2
k=k_[1]/k_[2]
m = (w03[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
plot(iris$Sepal.Length, iris$Sepal.Width, col = num2,
main="Iris Data prediction")
table(iris[,5],num2)
#Solve: w3*x+w03 = w1*x+w01
k_ = w1-w2
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
k_ = w1-w3
k=k_[1]/k_[2]
m = (w01[1,1]-w03[1,1])/k_[2]
abline(-m, -k)
k_ = w3-w2
k=k_[1]/k_[2]
m = (w03[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
k_ = w1-w2
k=k_[1]/k_[2]
m = (w01[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
k
m
k_ = w1-w3
k=k_[1]/k_[2]
m = (w01[1,1]-w03[1,1])/k_[2]
abline(-m, -k)
k
m
k_ = w3-w2
k=k_[1]/k_[2]
m = (w03[1,1]-w02[1,1])/k_[2]
abline(-m, -k)
k
m
